{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1223b56",
   "metadata": {},
   "source": [
    "### Test Docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd67bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:382: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "  warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "c:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Could not load the custom kernel for multi-scale deformable attention: Command '['where', 'cl']' returned non-zero exit status 1.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "file = '../../data/papers/Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf'\n",
    "converter = DocumentConverter()\n",
    "document = converter.convert(file)\n",
    "\n",
    "print(document.document.origin.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bac0d",
   "metadata": {},
   "source": [
    "### Test Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fd13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_MODEL=\"BAAI/bge-m3\"\n",
    "MAX_TOKENS=8192\n",
    "\n",
    "tokenizer = HuggingFaceTokenizer(\n",
    "        tokenizer=AutoTokenizer.from_pretrained(TOKENIZER_MODEL),\n",
    "        max_tokens=MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    merge_peers=True\n",
    ")\n",
    "\n",
    "chunk_iter = chunker.chunk(dl_doc=document.document)\n",
    "chunks = list(chunk_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e145f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schema_name': 'docling_core.transforms.chunker.DocMeta',\n",
       " 'version': '1.0.0',\n",
       " 'doc_items': [{'self_ref': '#/texts/2',\n",
       "   'parent': {'cref': '#/body'},\n",
       "   'children': [],\n",
       "   'content_layer': <ContentLayer.BODY: 'body'>,\n",
       "   'label': <DocItemLabel.TEXT: 'text'>,\n",
       "   'prov': [{'page_no': 1,\n",
       "     'bbox': {'l': 142.048,\n",
       "      't': 724.8820146484375,\n",
       "      'r': 455.715,\n",
       "      'b': 712.6010146484375,\n",
       "      'coord_origin': <CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>},\n",
       "     'charspan': (0, 61)}]},\n",
       "  {'self_ref': '#/texts/3',\n",
       "   'parent': {'cref': '#/body'},\n",
       "   'children': [],\n",
       "   'content_layer': <ContentLayer.BODY: 'body'>,\n",
       "   'label': <DocItemLabel.TEXT: 'text'>,\n",
       "   'prov': [{'page_no': 1,\n",
       "     'bbox': {'l': 72.983,\n",
       "      't': 710.9350146484375,\n",
       "      'r': 525.283,\n",
       "      'b': 698.6730146484375,\n",
       "      'coord_origin': <CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>},\n",
       "     'charspan': (0, 93)},\n",
       "    {'page_no': 1,\n",
       "     'bbox': {'l': 138.186,\n",
       "      't': 693.0410146484376,\n",
       "      'r': 460.079,\n",
       "      'b': 672.0820146484375,\n",
       "      'coord_origin': <CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>},\n",
       "     'charspan': (94, 200)}]}],\n",
       " 'headings': ['Evaluating the Text-to-SQL Capabilities of Large Language Models'],\n",
       " 'captions': None,\n",
       " 'origin': {'mimetype': 'application/pdf',\n",
       "  'binary_hash': 2355541156686147883,\n",
       "  'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "  'uri': None}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the documents metadata\n",
    "chunks[0].model_dump()['meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e58bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks = [\n",
    "    {\n",
    "        \"text\": chunk.text,\n",
    "        \"metadata\": {\n",
    "            \"filename\": chunk.meta.origin.filename,\n",
    "            \"page_numbers\": [\n",
    "                page_no\n",
    "                for page_no in sorted(\n",
    "                    set(\n",
    "                        prov.page_no\n",
    "                        for item in chunk.meta.doc_items \n",
    "                        for prov in item.prov\n",
    "                    )\n",
    "                )\n",
    "            ] or None,\n",
    "            \"title\": chunk.meta.headings[0] if chunk.meta.headings else None\n",
    "        },\n",
    "    }\n",
    "    for chunk in chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f5a7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Nitarshan Rajkumar 1 ∗ , Raymond Li 2 , Dzmitry Bahdanau 2345\\n1 University of Cambridge, 2 ServiceNow, 3 Mila, 4 McGill University, 5 Canada CIFAR AI Chair nr500@cam.ac.uk , {raymond.li,dzmitry.bahdanau}@servicenow.com https://github.com/nitarshan/codex-text2sql',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [1],\n",
       "   'title': 'Evaluating the Text-to-SQL Capabilities of Large Language Models'}},\n",
       " {'text': 'Weperform an empirical evaluation of Text-toSQLcapabilities of the Codex language model. We find that, without any finetuning , Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.\\nFinetuned, VA = . Finetuned, EX = . Finetuned, TS = . T5-base, VA = 72.7. T5-base, EX = 57.9. T5-base, TS = 54.5. T5-large, VA = 84.1. T5-large, EX = 67.2. T5-large, TS = 61.4. T5-3B, VA = 87.6. T5-3B, EX = 71.4. T5-3B, TS = 65.7. T5-3B ∗, VA = 88.2. T5-3B ∗, EX = 74.4. T5-3B ∗, TS = 68.3. T5-3B + PICARD ∗, VA = 97.8. T5-3B + PICARD ∗, EX = 79.1. T5-3B + PICARD ∗, TS = 71.7. BRIDGE v2 ∗, VA = -. BRIDGE v2 ∗, EX = 68.0. BRIDGE v2 ∗, TS = -. Inference-only, VA = . Inference-only, EX = . Inference-only, TS = . GPT-3 ada, VA = 33.8. GPT-3 ada, EX = 2.3. GPT-3 ada, TS = 0.3. GPT-3 babbage, VA = 48.8. GPT-3 babbage, EX = 5.7. GPT-3 babbage, TS = 3.9. GPT-3 curie, VA = 70.9. GPT-3 curie, EX = 12.6. GPT-3 curie, TS = 8.3. GPT-3 davinci, VA = 65.0. GPT-3 davinci, EX = 26.3. GPT-3 davinci, TS = 21.7. Codex cushman ∗, VA = 86.3. Codex cushman ∗, EX = 63.7. Codex cushman ∗, TS = 53.0. Codex davinci ∗, VA = 91.6. Codex davinci ∗, EX = 67.0. Codex davinci ∗, TS = 55.1',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [1],\n",
       "   'title': 'Abstract'}},\n",
       " {'text': 'Translating natural language questions to SQL queries (Text-to-SQL) is an important business problem which has seen significant research interest. A common approach to this task involves training a model to produce a SQL query when given a question, a database schema, and possibly database content as inputs. A clear trend in this area is to finetune models pretrained on natural language; notably, performance significantly improves as larger pretrained models are used (Shaw et al., 2021; Scholak et al., 2021).\\nRecent results from the broader field demonstrate that simply scaling training data and model size for generative language models brings advanced capabilities, such as few-shot learning without finetuning (GPT-3, Brown et al., 2020) and code generation (Codex, Chen et al., 2021). In this work we study if such models are already competitive Text-to-SQL solutions without any further finetuning on task-specific training data , evaluating Codex and GPT-3 models of different sizes with varied prompts on Text-to-SQL benchmarks.\\nWe find that Codex achieves a competitive performance of up to 67% execution accuracy on the Spider development set. We analyze the predicted queries that automatic evaluation judged as wrong\\n∗ Work partially done at Mila and the Université de Montréal.\\nTable 1: Best Spider development set performance across models, as measured by percentage of predictions which are valid SQL (VA), execution accuracy (EX), test-suite accuracy (TS). Models marked with ∗ use database content. T5 results are from Scholak et al. (2021), BRIDGE v2 results are from Lin et al. (2020).\\nand find that many of them would be judged correct by humans, whereas others could likely be fixed within the no-finetuning paradigm. Lastly, using GeoQuery and Scholar benchmarks we show that adapting Codex to a specific domain by prompting it with few examples can be more effective than fine-tuning a smaller language model on the same examples.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [1],\n",
       "   'title': '1 Introduction'}},\n",
       " {'text': 'Models Our evaluation focuses on the models accessible via the OpenAI API: GPT-3 (in the ascending ada, babbage, curie and davinci sizes) and Codex (in the ascending cushman-codex and davinci-codex sizes) 1 . These are generative language models which perform next-token prediction during training and inference; GPT-3 is trained on a diverse set of sources from the internet, and Codex is further finetuned on code from GitHub. We compare GPT-3 and Codex against methods from Shaw et al. (2021) using the T5 encoder-decoder\\n1 See Appendix A.2 for a discussion on parameter counts.\\nmodel. Starting from public checkpoints pretrained on Common Crawl, the T5 model is finetuned on Spider to predict the output SQL, conditioned on the question and schema. The 3B parameter T5 model is currently the state-of-the-art on Spider when combined with constrained inference using the PICARD algorithm (Scholak et al., 2021). We also compare to BRIDGE v2 (Lin et al., 2020), a sequence-to-sequence model based on BERT.\\nZero-Shot Experiments We use the Spider benchmark (Yu et al., 2019) for cross-domain Textto-SQL. We report performance using percentage of development set predictions which are valid (executable) SQLite SQL, execution accuracy, and test-suite execution accuracy. The latter metric was proposed by Zhong et al. (2020) to measure semantic equivalence of SQL queries written in different styles, which is essential when comparing Codex to models trained on Spider. We address concerns around possible memorization of Spider data by Codex in Appendix A.5.\\nFew-Shot Experiments We re-purpose the question-splits of the GeoQuery and Scholar datasets (Zelle and Mooney, 1996; Iyer et al., 2017; Finegan-Dollak et al., 2018) to perform experiments in a few-shot setting. The examples in these datasets are grouped by query templates. Examples corresponding to the same template have the same SQL query structure, but may have different English questions and SQL literals. To define the few-shot task, we first sort the templates by their frequency in the training set. In the n -shot setting we then use one random example for each of the n most frequent templates.\\nPrompts We use six prompt structures in our experiments (examples provided in Appendix C). Question provides no database information and just includes the question as a SQL comment. API Docs follows the style of the Text-to-SQL example in Codex documentation and includes a schema in a comment style which does not conform to SQLite standards. Select X includes in comments the results of executing a SELECT * FROM T LIMIT X query on each table, including schemas via column headers. Create Table includes the CREATE TABLE commands for each table, including column type and foreign key declarations. Create Table + Select X 2 is a combination of the\\n2 Only the davinci-codex model can evaluate Create Table + Select X prompts with more than 1 row, due to its expanded 4096-token prompt window compared to the 2048-token window of all other models. In addition, GPT-3 models prepro-\\nTable 2: Spider development set performance across prompt styles on the davinci-codex model, as measured by percentage of predictions which are valid SQL (V A), execution accuracy (EX), test-suite accuracy (TS).\\n\\nQuestion, VA = 14.0. Question, EX = 8.3. Question, TS = 8.2. API Docs, VA = 83.8. API Docs, EX = 56.8. API Docs, TS = 47.5. Select 1, VA = 86.3. Select 1, EX = 60.9. Select 1, TS = 52.0. Select 3, VA = 85.8. Select 3, EX = 60.3. Select 3, TS = 52.2. Select 5, VA = 85.2. Select 5, EX = 60.5. Select 5, TS = 51.5. Select 10, VA = 86.0. Select 10, EX = 60.8. Select 10, TS = 51.2. Create Table, VA = 89.8. Create Table, EX = 59.9. Create Table, TS = 50.0. + Select 1, VA = 92.5. + Select 1, EX = 64.8. + Select 1, TS = 53.7. + Select 3, VA = 91.6. + Select 3, EX = 67.0. + Select 3, TS = 55.1. + Select 5, VA = 91.0. + Select 5, EX = 65.3. + Select 5, TS = 53.9. + Select 10, VA = 91.2. + Select 10, EX = 63.3. + Select 10, TS = 52.4\\npreceding two prompt formats. Finally, Fewshot additionally includes question-query pairs.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [1, 2],\n",
       "   'title': '2 Experimental Setup'}},\n",
       " {'text': 'We present results for different model sizes in Table 1 and for different prompt styles in Table 2. Full results are available in Table 4 in Appendix B.\\nCodex provides a strong baseline for Text-toSQL tasks In Table 1 the best performing model (davinci-codex, Create Table + Select 3) achieves 67% execution accuracy and 56.5% test suite execution accuracy on Spider. This is comparable to the performance of the BRIDGE v2 (Lin et al., 2020) model which achieved a (then) state-of-the-art 68% execution accuracy in December 2020.\\nPrompt design is critical for performance As seen in Table 2, providing the question alone results in a low 8.3% execution accuracy. There is a progressive improvement to 56.8% as schema information is introduced in API Docs, to 59.9% when valid SQL and foreign key information is used in Create Table, and to 67.0% when database content is introduced with Create Table + Select 3.\\nMore database content can harm performance In Table 2 we observe that for the Select Limit X prompts there is a negligible change in performance when adding more rows. By contrast, Create Table + Select Limit X prompt accuracy peaks with 3 rows before significantly decreasing in performance as more rows are added.\\nDiminishing returns for Codex model size While GPT-3 performance significantly benefits from increased model size, the davinci-codex model does not perform drastically better than\\ncess whitespace tokens less efficiently than Codex models, and therefore cannot evaluate Create Table + Select X prompts at all.\\nFigure 1: Examples of error types, as made by the davinci-codex model with Create Table + Select 3 prompt. NL stands for natural language question. Percentage indicates the percentage of errors which are of the given type. Further examples are provided in Figure 3 in Appendix B.\\n\\n14%, 1 = Shortcuts. NL, 1 = What is the number of car models created by the car maker American Motor Company?. Gold, 1 = SELECT count(*) FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON T1.Id = T2.Maker WHERE T1.FullName = \\'American Motor Company\\';. Pred, 1 = SELECT COUNT(Model) FROM model_list WHERE Maker = 1;. NL, 1 = Give the city and country for the Alton airport.. Gold, 1 = SELECT City , Country FROM AIRPORTS WHERE AirportName = \"Alton\". Pred, 1 = SELECT City, Country FROM airports WHERE AirportCode = \\'ALN\\';. 8%, 1 = SELECT Extra Columns. NL, 1 = List names of conductors in descending order of years of work.. Gold, 1 = SELECT Name FROM conductor ORDER BY Year_of_Work DESC. Pred, 1 = SELECT Name, Year_of_Work FROM conductor ORDER BY Year_of_Work DESC;. 5%, 1 = SELECT Convention. NL, 1 = What are all the makers and models?. Gold, 1 = SELECT Maker , Model FROM MODEL_LIST;. Pred, 1 = SELECT DISTINCT car_makers.Maker, model_list.Model FROM car_makers JOIN model_list ON car_makers.Id = model_list.Maker;\\ncushman-codex. Full results in Table 4 in Appendix B show cushman-codex generally being within 1 percentage point of davinci-codex for the same prompt style; it even performs 3 percentage points better for the Create Table prompt. These results suggest that davinci-codex\\'s longer context window may be a greater contributor to its peak performance than increased parameter count.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [2, 3],\n",
       "   'title': '3 Zero-Shot Results'}},\n",
       " {'text': \"Test-Suite Correct, % = 55.1. Test-Suite Correct, E% = -. Semantic Incorrect, % = 25.2. Semantic Incorrect, E% = 69. - Shortcuts, % = 5.1. - Shortcuts, E% = 14. - GROUP BY Convention, % = 1.5. - GROUP BY Convention, E% = 4. - Other, % = 18.6. - Other, E% = 51. Ambiguous Correct, % = 11.3. Ambiguous Correct, E% = 31. - SELECT Extra Columns, % = 2.9. - SELECT Extra Columns, E% = 8. - SELECT Convention, % = 1.8. - SELECT Convention, E% = 5. - Argmax, % = 1.5. - Argmax, E% = 4. - Other, % = 5.1. - Other, E% = 14. Invalid SQL, % = 8.4. Invalid SQL, E% = -. - Ambiguous column name, % = 1.9. - Ambiguous column name, E% = -. - No such column, % = 4.5. - No such column, E% = -\\nWe focus our error analysis on the davinci-codex model with Create Table + Select 3 prompt, and present a breakdown of prediction types in Table 3 and examples of errors in Figure 1. Our error categories were chosen to surface the most interesting Codex-specific behaviours we observed amongst the errors made. We randomly selected and annotated 100 predictions which were valid SQL yet were judged incorrect by test-suite evaluation.\\nWe first consider Semantic Incorrect behaviours, which Spider evaluation and the human annotator both view as incorrect predictions. Shortcut errors are where Codex made use of either specific table values or 'world knowledge' from GPT-3 pretraining, while the ground-truth query contained the exact literals from the question. GROUP BY Convention errors are where Codex incorrectly groups on a non-primary-key column (such as a name or title column).\\nWe also consider Ambiguous Correct behaviours which are semantically different from the gold query and are therefore judged as incorrect by Spider evaluation, but which the human annotator viewed as being an acceptable SQL translation of\\nTable 3: Breakdown of prediction annotations over Spider development set for the davinci-codex model with Create Table + Select 3 prompt. % is percentage of all predictions, E% is percentage of manually annotated erroneous queries (see Section Section 3.1 for details).\\nthe given question. SELECT Convention errors are where Codex selects a different column than the per-database convention of the gold queries (such as name instead of ID). SELECT Extra Columns errors are where Codex includes additional useful columns in its query beyond what the gold query includes. Argmax errors are where Codex differs from the gold query in how a min/max resolution (such as 'youngest singer') is handled for ties.\\nWe observe in Table 3 that a significant 31% of valid yet erroneous predictions are penalized by Spider evaluation as being incorrect though a human annotator viewed them as acceptable solutions. Future work could be to investigate to what extent one can control the behaviour of Codex. This could allow to fix these ambiguous errors, either by prompt design or using a few examples.\",\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [3],\n",
       "   'title': '3.1 Error Analysis'}},\n",
       " {'text': 'We investigate whether Codex can perform fewshot Text-to-SQL. As described in Section 2, we re-purpose the GeoQuery and Sholar datasets in a few-shot setting. It is well known that models trained on Spider transfer poorly to other singledatabase Text-to-SQL datasets (Suhr et al., 2020) in a zero-shot setting. Studying few-shot Text-to-SQL on GeoQuery and Scholar should show to what extent models are able to leverage a small amount of examples to effectively adapt to a new domain.\\nBaseline The baseline is a T5-3B model that was finetuned on Spider, reaching 71% exact-match accuracy on Spider validation set. The model is then further finetuned on the new domain - GeoQuery or Scholar. The learning rate for domainspecific-finetuning was selected in the 20 -shot setting among [0 . 1 , 0 . 2 , 0 . 5 , 1 , 2] · 10 -5 , based on the best validation set performance after 300 steps. We use batch-size 1024, such that all the few-shot examples fit in the same batch.\\nCodex Building on the Create Table + Select X prompt, we append n question-query examples to the input in an n -shot setting. An example of this prompt is provided in Figure 11. All samples are generated using greedy decoding, with temperature 0 . Note that for a given n -shot setting, the baseline and Codex use the same set of support examples. These examples are in the prompt for Codex, and used to finetune the baseline on the new domain. Given the limited window-size of API models, on GeoQuery we can feed up to 40 support examples to davinci-codex, and up to 10 examples to cushman-codex and GPT-3 models. On Scholar the queries are longer and the schema more complex we fit only 10 examples in the prompt of davincicodex, 5 for cushman-codex, and none at all for GPT-3 models.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [4],\n",
       "   'title': '4 Few-Shot'}},\n",
       " {'text': 'Figure 2 shows test-suite accuracies on the Scholar and GeoQuery datasets. The baseline reaches 85.7% test-set performance when trained on the complete GeoQuery training set (549 examples). Respectively, it reaches 87.2% test accuracy when trained on the whole Scholar training set (499 examples). This simple baseline is a very competitive model when considering the entire datasets. However Figure 2 shows that it is largely beaten by Codex in few-shot settings. In a zero-shot setting, both davinci-codex and cushman-codex al-\\n(a) GeoQuery. When trained on the whole GeoQuery training set (549 examples), the finetuned T5 reaches 85.7% accuracy.\\n(b) Scholar. When trained on the whole Scholar training set (499 examples), the finetuned T5 reaches 87.2% accuracy.\\nFigure 2: Test-suite accuracy with varying number of support examples. The x-axis shows the number of fewshot examples used.\\nready beat the baseline on GeoQuery. We speculate that Codex performs well here because it uses the same argmax convention as the GeoQuery dataset, which is different than the convention used in Spider. With up to 40 examples in the prompt, davinci-codex outperforms a T5-3B model finetuned on these same examples by a large margin, whereas GPT-3 davinci performs quite poorly on this task. On the other hand, the T5 model outperforms Codex in a zero-shot setting on Scholar. In 5 and 10-shot settings, Codex shows better adaptation from these few samples and beats the T5 baseline.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [4],\n",
       "   'title': '4.1 Results'}},\n",
       " {'text': 'We demonstrated that generative language models trained on code provide a strong baseline for Text-to-SQL. We also provided analysis of failure modes for these models, which we hope guides further prompt design (whether few-shot or through natural language instructions) in this setting. Finally, we showed that prompt-based few-shot learning with these models performs competitively with finetuning-based few-shot learning of smaller models. A clear direction for future work is to evaluate the benefits of finetuning with Codex models.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [4],\n",
       "   'title': '5 Conclusion'}},\n",
       " {'text': 'Nitarshan performed all zero-shot and finetuning experiments as well as error-analysis, and wrote most of the paper. Raymond performed all few-shot experiments and the associated writing. Dzmitry supervised, and contributed to paper editing.\\nWe thank Dóra Jámbor for insightful discussions, Laurent Charlin for providing funding for Nitarshan and for providing feedback on this work, Fraser Kelton and Dave Cummings for support with the OpenAI API, and Ruiqi Zhong for assistance with Spider test suites. We also thank anonymous ARR reviewers for their feedback and criticism in the review process.\\nNitarshan additionally thanks the city of Montréal and its cafés for providing inspirational settings in which to conduct this work.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [5],\n",
       "   'title': 'Acknowledgements'}},\n",
       " {'text': 'Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.\\nXiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. 2021. Structure-grounded pretraining for text-to-sql. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies .\\nCatherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018. Improving text-to-SQL evaluation methodology. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 351-360, Melbourne, Australia. Association for Computational Linguistics.\\nLeo Gao. 2021. On the Sizes of OpenAI API Models.\\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learning a neural semantic parser from user feedback. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 963-973, Vancouver, Canada. Association for Computational Linguistics.\\nXi Victoria Lin, Richard Socher, and Caiming Xiong. 2020. Bridging textual and tabular data for crossdomain text-to-sql semantic parsing.\\nTorsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. Picard: Parsing incrementally for constrained auto-regressive decoding from language models.\\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2021. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 922-938, Online. Association for Computational Linguistics.\\nAlane Suhr, Ming-Wei Chang, Peter Shaw, and Kenton Lee. 2020. Exploring unexplored generalization challenges for cross-database semantic parsing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 83728388, Online. Association for Computational Linguistics.\\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2019. Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task.\\nJohn M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2 , pages 1050-1055.\\nRuiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic evaluation for text-to-sql with distilled test suites.\\nAlbert Ziegler. 2021. Research recitation.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [5],\n",
       "   'title': 'References'}},\n",
       " {'text': 'At time of writing, the OpenAI API was accessible at https://openai.com/api/ . The example from which our API Docs prompt draws from can be found at https://beta.openai.com/examples/ default-sql-translate .',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [6],\n",
       "   'title': 'A API Details'}},\n",
       " {'text': \"We sample 200 tokens from GPT-3 and Codex with temperature 0, with the following strings used as stop tokens to halt generation: '--', '\\\\n\\\\n', ';', '#'.\",\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [6],\n",
       "   'title': 'A.1 Hyperparameters'}},\n",
       " {'text': \"Parameter counts for OpenAI API models are not openly available. Gao (2021) evaluated API GPT3 models across a variety of language modelling tasks to compare to published results in Brown et al. (2020), finding that 'Ada, Babbage, Curie and Davinci line up closely with 350M, 1.3B, 6.7B, and 175B respectively'. We presume that the davincicodex model is the same size as the GPT-3 davinci model; cushman-codex is a new model name so we can only guess that it is of a similar (but not the same) size to GPT-3 curie. Nevertheless these remain guesses which should not be relied on.\",\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [6],\n",
       "   'title': 'A.2 Parameter Counts'}},\n",
       " {'text': 'The exact models served through the OpenAI API may vary over time. We verified that for each model type, only a single model version was used to generate results. These versions are ada:2020-05-03 , babbage:2020-05-03 , curie:2020-05-03 , davinci:2020-05-03 , cushman-codex:2021-08-03 , davinci-codex:2021-08-03 .',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [6],\n",
       "   'title': 'A.3 Model Versioning'}},\n",
       " {'text': 'In Table 4 we include preliminary results from finetuning GPT-3 models on the Spider training set. We used the full training set, and the default finetuning settings of 4 epochs, a batch size of 8, and a learning rate multiplier of 0.1. We did not perform a hyperparameter sweep due to the significant cost this would incur.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [6],\n",
       "   'title': 'A.4 Finetuning'}},\n",
       " {'text': 'The Spider development set is available on GitHub, and is therefore possibly in the training set of Codex. We believe that this does not manifest as memorization for our results however, for the following reasons.\\nEvaluation data on Spider\\'s repo is formatted differently to our prompts. Most related is the dev.sql file, which contains evaluation questionquery pairs in the following format:\\n```\\nQuestion 1: ... SQL: ...\\n```\\n...\\nThis resembles but isn\\'t identical to our \"Question\" prompt. We prompted Codex with verbatim fragments of this file and generations failed to replicate any file contents. Our \"Question\" prompt has very poor performance - hardly an indication of memorization from dev.sql . Furthermore, most of Codex\\'s performance is due to including in the prompt the schemas (see Table 2), which are not present in dev.sql .\\nAs well, Codex prediction style is very different to evaluation gold queries. Gold queries make use of a consistent table aliasing strategy (using T1, T2, etc.) which we never see with Codex (see Figure 3 for example comparisons).\\nFurthermore, in Table 4 we reported performance for all models on spider-realistic (Deng et al., 2021), a modification of the spider evaluation set that removes column name references in questions. We observe a similar trend in performance across models as on spider (the consistent performance drop on spider-realistic is expected due to the difficulty of the updated dataset). Memorization cannot account for the performance observed, as spider-realistic is not publicly available on GitHub.\\nFinally, Ziegler (2021) studied memorization in Copilot, a derivative of the Codex models, and found that \"Copilot can quote a body of code verbatim, but that it rarely does so, and when it does, it mostly quotes code that everybody quotes, and mostly at the beginning of a file\". Spider evaluation data is rare on GitHub, and we use long contexts in our prompts that significantly differ from the files on GitHub.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [6],\n",
       "   'title': 'A.5 Memorization'}},\n",
       " {'text': 'We chose not to evaluate on the held-out test set of Spider, as this could not be done offline - it would instead require sending these held-out examples through the API to OpenAI, which risks inadvertently leaking them for retraining of Codex.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [6],\n",
       "   'title': 'A.6 Choice of Spider Evaluation Set'}},\n",
       " {'text': 'GPT-3, Prompt = . GPT-3, VA = . GPT-3, EX = . GPT-3, TS = . ada, Prompt = Question. ada, VA = 1.2 (1.0). ada, EX = 0.0 (0.0). ada, TS = 0.0 (0.0). ada, Prompt = Docs. ada, VA = 3.4 (2.2). ada, EX = 0.2 (0.2). ada, TS = 0.1 (0.0). ada, Prompt = 1 Row. ada, VA = 40.1 (34.6). ada, EX = 1.1 (0.6). ada, TS = 0.2 (0.0). ada, Prompt = Schema. ada, VA = 33.8 (33.9). ada, EX = 2.3 (3.5). ada, TS = 0.3 (0.0). babbage, Prompt = Question. babbage, VA = 4.4 (2.0). babbage, EX = 1.0 (0.2). babbage, TS = 1.0 (0.2). babbage, Prompt = Docs. babbage, VA = 22.5 (20.3). babbage, EX = 1.0 (0.6). babbage, TS = 0.7 (0.2). babbage, Prompt = 1 Row. babbage, VA = 56.0 (49.8). babbage, EX = 5.1 (1.6). babbage, TS = 3.9 (0.0). babbage, Prompt = Schema. babbage, VA = 48.8 (44.9). babbage, EX = 5.7 (0.8). babbage, TS = 3.9 (0.0). curie, Prompt = Question. curie, VA = 9.0 (6.7). curie, EX = 2.9 (2.4). curie, TS = 2.5 (1.8). curie, Prompt = Docs. curie, VA = 25.2 (25.0). curie, EX = 7.4 (5.5). curie, TS = 6.3 (3.3). curie, Prompt = 1 Row. curie, VA = 70.6 (67.3). curie, EX = 10.8 (7.3). curie, TS = 7.6 (1.4). curie, Prompt = Schema. curie, VA = 70.9 (72.2). curie, EX = 12.6 (11.0). curie, TS = 8.3 (4.1). davinci, Prompt = Schema. davinci, VA = 65.0 (65.4). davinci, EX = 26.3 (23.2). davinci, TS = 21.7 (14.2). Finetuned, Prompt = GPT-3. Finetuned, VA = . Finetuned, EX = . Finetuned, TS = . ada, Prompt = Schema. ada, VA = 27.5 (21.3). ada, EX = 20.2 (14.0). ada, TS = 19.1 (13.0). babbage, Prompt = Schema. babbage, VA = 47.2 (38.0). babbage, EX = 34.8 (23.6). babbage, TS = 31.9 (20.9). curie, Prompt = Schema. curie, VA = 66.9 (60.2). curie, EX = 51.3 (37.8). curie, TS = 46.9 (32.9). Codex, Prompt = . Codex, VA = . Codex, EX = . Codex, TS = . cushman, Prompt = Question. cushman, VA = 11.3 (8.1). cushman, EX = 8.5 (3.9). cushman, TS = 8.3 (3.9). cushman, Prompt = Docs. cushman, VA = 83.8 (80.5). cushman, EX = 53.2 (45.1). cushman, TS = 43.5 (32.3). cushman, Prompt = 1 Row. cushman, VA = 84.7 (80.9). cushman, EX = 59.6 (49.2). cushman, TS = 48.5 (32.5). cushman, Prompt = 3 Rows. cushman, VA = 82.9 (79.1). cushman, EX = 60.3 (49.2). cushman, TS = 49.4 (33.7). cushman, Prompt = 5 Rows. cushman, VA = 83.6 (78.3). cushman, EX = 61.5 (49.6). cushman, TS = 50.4 (33.9). cushman, Prompt = Schema. cushman, VA = 88.3 (83.1). cushman, EX = 62.1 (49.6). cushman, TS = 53.1 (36.2). cushman, Prompt = + 1 Row. cushman, VA = 86.3 (85.0). cushman, EX = 63.7 (54.9). cushman, TS = 53.0 (39.6). davinci, Prompt = Question. davinci, VA = 14.0 (8.9). davinci, EX = 8.3 (4.5). davinci, TS = 8.2 (4.1). davinci, Prompt = Docs. davinci, VA = 83.8 (87.4). davinci, EX = 56.8 (51.8). davinci, TS = 47.5 (39.0). davinci, Prompt = 1 Row. davinci, VA = 86.3 (83.5). davinci, EX = 60.9 (54.7). davinci, TS = 52.0 (41.3). davinci, Prompt = 3 Rows. davinci, VA = 85.8 (82.7). davinci, EX = 60.3 (53.3). davinci, TS = 52.2 (40.0). davinci, Prompt = 5 Rows. davinci, VA = 85.2 (80.9). davinci, EX = 60.5 (51.4). davinci, TS = 51.5 (38.4). davinci, Prompt = 10 Rows. davinci, VA = 86.0 (80.7). davinci, EX = 60.8 (53.3). davinci, TS = 51.2 (39.2). davinci, Prompt = Schema. davinci, VA = 89.8 (87.8). davinci, EX = 59.9 (52.2). davinci, TS = 50.0 (38.4). davinci, Prompt = + 1 Row. davinci, VA = 92.5 (90.7). davinci, EX = 64.8 (58.7). davinci, TS = 53.7 (41.7). davinci, Prompt = + 3 Rows. davinci, VA = 91.6 (90.6). davinci, EX = 67.0 (60.2). davinci, TS = 55.1 (42.9). davinci, Prompt = + 5 Rows. davinci, VA = 91.0 (90.2). davinci, EX = 65.3 (60.0). davinci, TS = 53.9 (42.1). davinci, Prompt = + 10 Rows. davinci, VA = 91.2 (90.6). davinci, EX = 63.3 (58.3). davinci, TS = 52.4 (40.2)\\nTable 4: Performance on Spider across all evaluated models and prompts, as measured by percentage of predictions which are valid/executable SQL (VA), execution accuracy (EX), test-suite accuracy (TS). Main results are on the Spider development set, results in parantheses are on Spider-Realistic (Deng et al., 2021), a modified subset of the Spider development set with explicit references to column names removed from questions.\\n14%, 1 = Shortcuts. Q, 1 = What are the template ids with template type description \"Presentation\".. Gold, 1 = SELECT T2.template_id FROM Ref_template_types AS T1 JOIN Templates AS T2 ON T1.template_type_code = T2.template_type_code WHERE T1.template_type_description = \"Presentation\". Pred, 1 = SELECT Template_ID FROM Templates WHERE Template_Type_Code = \\'PPT\\';. 8%, 1 = SELECT Extra Columns. NL, 1 = Which city has the most frequent destination airport?. Gold, 1 = SELECT T1.City FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode = T2.DestAirport GROUP BY T1.City ORDER BY count(*) DESC LIMIT 1. Pred, 1 = SELECT City, COUNT(*) AS \\'Number of Destinations\\' FROM airports JOIN flights ON airports.AirportCode = flights.DestAirport GROUP BY City ORDER BY COUNT(*) DESC LIMIT 1;. 5%, 1 = SELECT Convention. NL, 1 = Which airlines have a flight with source airport AHD?. Gold, 1 = SELECT T1.Airline FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T1.uid = T2.Airline WHERE T2.SourceAirport = \"AHD\". Pred, 1 = SELECT Airline FROM flights WHERE SourceAirport = \\'AHD\\';. 4%, 1 = GROUP BY Convention. NL Gold, 1 = Which makers designed more than 3 car models? List full name and the id. SELECT T1.FullName , T1.Id FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON. Pred, 1 = T1.Id = T2.Maker GROUP BY T1.Id HAVING count(*) > 3; SELECT car_makers.FullName, car_makers.Id FROM car_makers JOIN model_list ON car_makers.Id = model_list.Maker GROUP BY car_makers.FullName HAVING. , 1 = COUNT(model_list.Model) > 3;. , 1 = Argmax. 4% NL, 1 = Give the name, year of independence, and surface area of the country that has the lowest population.. Gold Pred, 1 = SELECT Name, SurfaceArea, IndepYear FROM country ORDER BY Population LIMIT 1 SELECT Name, IndepYear, SurfaceArea FROM country WHERE Population = (SELECT. 51%, 1 = Other - Semantic Incorrect Which countries have either. NL, 1 = English. Gold, 1 = or Dutch as an official language? SELECT * FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = \"English\" AND IsOfficial = \"T\" UNION SELECT * FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = \"Dutch\" AND IsOfficial = \"T\". Pred, 1 = SELECT * FROM countrylanguage WHERE IsOfficial = \\'T\\' OR IsOfficial = \\'F\\'; Show the name and the release year of the song by the youngest singer.. NL, 1 = SELECT song_name , song_release_year FROM singer ORDER. Gold Pred, 1 = BY age LIMIT 1 SELECT Name, Song_release_year FROM singer WHERE Age = (SELECT MIN(Age) FROM. , 1 = singer);. NL, 1 = What is the maximum miles per gallon of the car with 8 cylinders or produced before 1980 ? select max(mpg) from cars_data where cylinders = 8 or year <. Gold Pred, 1 = 1980 SSELECT MAX(MPG) FROM cars_data WHERE Cylinders = 8 AND Year < 1980;. 14%, 1 = Other - Ambiguous Correct. NL, 1 = Find the first name and age of students who have a pet. SELECT DISTINCT T1.fname , T1.age FROM. Gold, 1 = student AS T1 JOIN has_pet AS T2 ON T1.stuid = T2.stuid. Pred, 1 = SELECT Fname, Age FROM Student WHERE StuID IN (SELECT StuID FROM. , 1 = Has_Pet); How many countries speak both English and Dutch?. NL, 1 = SELECT COUNT(*) FROM (SELECT T1.Name FROM. Gold, 1 = country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = \"English\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = \"Dutch\"). Pred, 1 = SELECT COUNT(*) FROM countrylanguage WHERE Language = \\'Dutch\\' AND CountryCode IN (SELECT CountryCode FROM countrylanguage WHERE Language = \\'English\\');. NL, 1 = What are the Asian countries which have a population larger than that of any country in Africa? SELECT Name FROM country WHERE Continent = \"Asia\" AND population > (SELECT min(population) FROM country WHERE Continent = \"Africa\"). Gold, 1 = SELECT Name FROM country WHERE Continent = \\'Asia\\' AND Population > (SELECT. Pred, 1 = MAX(Population) FROM country WHERE Continent = \\'Africa\\');\\nFigure 3: Additional examples of error types, as made by davinci-codex model with Create Table + Select 3 prompt. NL stands for natural language question. Percentage indicates the percentage of errors which are of the given type.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [7, 8],\n",
       "   'title': 'B Additional Tables and Figures'}},\n",
       " {'text': 'Question Gold',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [9],\n",
       "   'title': '0-shot pred 10-shot pred'}},\n",
       " {'text': 'what states does the missouri river run through SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"missouri\" ; -what is the size of texas SELECT STATEalias0.AREA FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"texas\" ; -what are the major cities in texas SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 AND CITYalias0.STATE_NAME = \"texas\" ; -what is the capital of pennsylvania SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"pennsylvania\" ; - what is the biggest city in nebraska SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"nebraska\" ) AND CITYalias0.STATE_NAME = \"nebraska\" ; -what is the population of austin SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"austin\" ; -which state is kalamazoo in SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"kalamazoo\" ; -name all the rivers in colorado SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"colorado\" ; -what states border missouri SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"missouri\" ; - how many people live in new mexico SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"new mexico\" ;',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [9],\n",
       "   'title': '10-shot examples'}},\n",
       " {'text': 'which states border iowa\\nSELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"iowa\" ; SELECT state_name FROM border WHERE border = \\'iowa\\' SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"iowa\"',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [9],\n",
       "   'title': 'Very similar query in the few-shot prompt fixes the example'}},\n",
       " {'text': 'what state has the smallest population\\nSELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = (SELECT MIN(STATEalias1.POPULATION) FROM STATE\\nAS STATEalias1) ;\\nSELECT state_name FROM state ORDER BY population LIMIT 1 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = (SELECT MIN(STATEalias1.POPULATION) FROM STATE\\nAS STATEalias1)',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [9],\n",
       "   'title': 'Argmax convetion fixed'}},\n",
       " {'text': 'Question Gold\\nwhat is the population of the state with the largest area\\nSELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = (SELECT MAX(STATEalias1.AREA) FROM STATE AS STATEalias1) ;\\n0-shot pred\\nSELECT state_name, population FROM state WHERE area = (SELECT MAX(area) FROM state)\\n10-shot pred\\nSELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = (SELECT MAX(STATEalias1.AREA) FROM STATE AS STATEalias1)\\nFigure 4: Cherry-picked examples of Codex improvements from 0-shot to 10-shot text-to-SQL on GeoQuery validation set. The style of the generated SQL changes a lot and is much closer to that of the gold SQL when few-shot examples are in the prompt. The few-shot examples were also useful to adapt the generated SQL to the conventions of the dataset, like the way argmax is done, or the selected columns.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [9],\n",
       "   'title': 'SELECT extra columns fixed'}},\n",
       " {'text': \"```\\nWhat is Kyle's id? | network_1 | highschooler : id, name ( Kyle ), grade | friend : student_id, friend_id | likes : student_id, liked_id Figure 5: Example input for baseline T5 models. -- Using valid SQLite, answer the following questions. -- What is Kyle's id? SELECT Figure 6: Example prompt for Question . ### SQLite SQL tables, with their properties: # # Highschooler(ID, name, grade) # Friend(student_id, friend_id) # Likes(student_id, liked_id) # ### What is Kyle's id? SELECT\\n```\\nFigure 7: Example prompt for API Docs .\\n```\\n/* 3 example rows from table Highschooler: SELECT * FROM Highschooler LIMIT 3; Table: Highschooler ID name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ /* 3 example rows from table Friend: SELECT * FROM Friend LIMIT 3; Table: Friend student_id friend_id 1510 1381 1510 1689 1689 1709 */ /* 3 example rows from table Likes: SELECT * FROM Likes LIMIT 3; Table: Likes student_id liked_id 1689 1709 1709 1689 1782 1709 */ -- Using valid SQLite, answer the following questions for the tables provided above. -- What is Kyle's id? SELECT\\n```\",\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [10, 11],\n",
       "   'title': 'C Example Prompts'}},\n",
       " {'text': \"```\\nCREATE TABLE Highschooler( ID int primary key, name text, grade int) CREATE TABLE Friend( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references Highschooler(ID), foreign key (friend_id) references Highschooler(ID) ) CREATE TABLE Likes( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references Highschooler(ID), foreign key (student_id) references Highschooler(ID) ) -- Using valid SQLite, answer the following questions for the tables provided above. -- What is Kyle's id?\\n```\\n```\\nSELECT\\n```\",\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [11],\n",
       "   'title': 'Figure 8: Example prompt for Select 3 .'}},\n",
       " {'text': '```\\nCREATE TABLE Highschooler( ID int primary key, name text, grade int) /* 3 example rows: SELECT * FROM Highschooler LIMIT 3; ID name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ CREATE TABLE Friend( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references Highschooler(ID), foreign key (friend_id) references Highschooler(ID) ) /* 3 example rows: SELECT * FROM Friend LIMIT 3; student_id friend_id 1510 1381 1510 1689 1689 1709 */ CREATE TABLE Likes( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references Highschooler(ID), foreign key (student_id) references Highschooler(ID) ) /* 3 example rows: SELECT * FROM Likes LIMIT 3; student_id liked_id 1689 1709 1709 1689 1782 1709\\n```\\n\\nFigure 10: Example prompt for Create Table + Select 3 .\\n```\\n*/ -- Using valid SQLite, answer the following questions for the tables provided above. -- What is Kyle\\'s id? SELECT\\n```\\nCREATE TABLE \"border_info\" (\"state_name\" text, \"border\" text)\\n/*\\nstate_name, 1 = border. alabama, 1 = tennessee. alabama, 1 = georgia. alabama, 1 = florida\\n*/\\nCREATE TABLE \"city\" (\"city_name\" text, \"population\" int DEFAULT NULL, \"country_name\" varchar(3) NOT NULL DEFAULT \\'\\', \" state_name\" text)\\n/*\\ncity_name, 1 = population. city_name, 2 = country_name. city_name, 3 = state_name. birmingham, 1 = 284413. birmingham, 2 = usa. birmingham, 3 = alabama. mobile, 1 = 200452. mobile, 2 = usa. mobile, 3 = alabama. montgomery, 1 = 177857. montgomery, 2 = usa. montgomery, 3 = alabama\\n*/\\nCREATE TABLE \"highlow\" (\"state_name\" text, \"highest_elevation\" text, \"lowest_point\" text, \"highest_point\" text, \" lowest_elevation\" text)\\n/*\\n*/\\nCREATE TABLE \"lake\" (\"lake_name\" text, \"area\" double DEFAULT NULL, \"country_name\" varchar(3) NOT NULL DEFAULT \\'\\', \"state_name\" text)\\n/*\\nlake_name, 1 = area. lake_name, 2 = country_name. lake_name, 3 = state_name. iliamna, 1 = 2675.0. iliamna, 2 = usa. iliamna, 3 = alaska. becharof, 1 = 1186.0. becharof, 2 = usa. becharof, 3 = alaska. teshekpuk, 1 = 816.0. teshekpuk, 2 = usa. teshekpuk, 3 = alaska\\nCREATE TABLE \"mountain\" (\"mountain_name\" text, \"mountain_altitude\" int DEFAULT NULL, \"country_name\" varchar(3) NOT NULL DEFAULT \\'\\', \"state_name\" text)\\n/*\\n*/\\nst. elias, mountain_altitude 6194 = 5489. st. elias, country_name usa = usa. st. elias, state_name alaska = alaska. foraker, mountain_altitude 6194 = 5304. foraker, country_name usa = usa. foraker, state_name alaska = . , mountain_altitude 6194 = . , country_name usa = . , state_name alaska = alaska\\nCREATE TABLE \"river\" (\"river_name\" text, \"length\" int DEFAULT NULL, \"country_name\" varchar(3) NOT NULL DEFAULT \\'\\', \"traverse\" text)\\n/*\\nriver_name mississippi\\nmississippi mississippi\\nlength\\n3778\\n3778\\n3778\\ncountry_name traverse\\nusa minnesota usa wisconsin\\nusa iowa\\n*/\\nCREATE TABLE \"state\" (\"state_name\" text, \"population\" int DEFAULT NULL, \"area\" double DEFAULT NULL, \"country_name\" varchar(3) NOT NULL DEFAULT \\'\\', \"capital\" text, \"density\" double DEFAULT NULL)\\n/*\\n*/\\nstate_name, 1 = population. state_name, 2 = area. state_name, 3 = country_name. state_name, 4 = capital density. alabama, 1 = 3894000. alabama, 2 = 51700.0. alabama, 3 = usa. alabama, 4 = montgomery 75.319149. alaska, 1 = 401800. alaska, 2 = 591000.0. alaska, 3 = usa. alaska, 4 = juneau 0.679865. arizona, 1 = 2718000. arizona, 2 = 114000.0. arizona, 3 = usa. arizona, 4 = phoenix 23.842105\\n- --Using valid SQLite, answer the following questions for the tables provided above.\\n-- what is the population\\nof austin\\nSELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"austin\" ;\\n-- which\\nstate is kalamazoo in\\nSELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"kalamazoo\" ;\\n-- name all\\nthe rivers\\nin colorado\\nSELECT\\nRIVERalias0.RIVER_NAME\\n-- how\\nSELECT\\nFROM\\nRIVER\\nmany people\\nlive in new mexico\\nSTATEalias0.POPULATION\\nFROM\\nSTATE\\nAS\\nAS\\nRIVERalias0 WHERE\\nSTATEalias0 WHERE\\nRIVERalias0.TRAVERSE =\\nSTATEalias0.STATE_NAME\\n\"colorado\"\\n;\\n=\\n\"new mexico\"\\n;\\n-- what states border missouri\\nSELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"missouri\" ;\\n-- what is the biggest\\ncity in arizona\\nSELECT\\nFigure 11: Example prompt for 5-shot . It starts with the schema and 3 rows per database (exactly as in Figure 10), followed by 5 few-shot examples, and finally the target question.',\n",
       "  'metadata': {'filename': 'Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf',\n",
       "   'page_numbers': [12, 13],\n",
       "   'title': 'Figure 9: Example prompt for Create Table .'}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de92b7",
   "metadata": {},
   "source": [
    "### Test LanceDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from typing import List\n",
    "db = lancedb.connect(\"data/lancedb\")\n",
    "\n",
    "func = get_registry().get(\"huggingface\").create(name=\"BAAI/bge-m3\")\n",
    "\n",
    "class ChunkMetadata(LanceModel):\n",
    "    filename: str | None\n",
    "    page_numbers: List[int] | None\n",
    "    title: str | None\n",
    "\n",
    "class Chunks(LanceModel):\n",
    "    text: str = func.SourceField()\n",
    "    vector: Vector(func.ndims()) = func.VectorField()\n",
    "    metadata: ChunkMetadata\n",
    "\n",
    "table = db.create_table(\"docling\", schema=Chunks, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "629a4b96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Field 'title' not found in target schema",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_chunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lancedb\\table.py:2174\u001b[0m, in \u001b[0;36mLanceTable.add\u001b[1;34m(self, data, mode, on_bad_vectors, fill_value)\u001b[0m\n\u001b[0;32m   2144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[0;32m   2145\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2146\u001b[0m     data: DATA,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     fill_value: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m   2150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AddResult:\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add data to the table.\u001b[39;00m\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;124;03m    If vector columns are missing and the table\u001b[39;00m\n\u001b[0;32m   2153\u001b[0m \u001b[38;5;124;03m    has embedding functions, then the vector columns\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2172\u001b[0m \u001b[38;5;124;03m        The number of vectors in the table.\u001b[39;00m\n\u001b[0;32m   2173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLOOP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_bad_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lancedb\\background_loop.py:25\u001b[0m, in \u001b[0;36mBackgroundEventLoop.run\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_coroutine_threadsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lancedb\\table.py:3329\u001b[0m, in \u001b[0;36mAsyncTable.add\u001b[1;34m(self, data, mode, on_bad_vectors, fill_value)\u001b[0m\n\u001b[0;32m   3327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3328\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m-> 3329\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43m_sanitize_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_bad_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_bad_vectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_subschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pa\u001b[38;5;241m.\u001b[39mTable):\n\u001b[0;32m   3338\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_reader()\n",
      "File \u001b[1;32mc:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lancedb\\table.py:268\u001b[0m, in \u001b[0;36m_sanitize_data\u001b[1;34m(data, target_schema, metadata, on_bad_vectors, fill_value, allow_subschema)\u001b[0m\n\u001b[0;32m    265\u001b[0m     target_schema \u001b[38;5;241m=\u001b[39m target_schema\u001b[38;5;241m.\u001b[39mwith_metadata(new_metadata)\n\u001b[0;32m    267\u001b[0m _validate_schema(target_schema)\n\u001b[1;32m--> 268\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43m_cast_to_target_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_subschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reader\n",
      "File \u001b[1;32mc:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lancedb\\table.py:285\u001b[0m, in \u001b[0;36m_cast_to_target_schema\u001b[1;34m(reader, target_schema, allow_subschema)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m==\u001b[39m target_schema:\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# Fast path when the schemas are already the same\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reader\n\u001b[1;32m--> 285\u001b[0m fields \u001b[38;5;241m=\u001b[39m \u001b[43m_align_field_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtarget_schema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m reordered_schema \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mschema(fields, metadata\u001b[38;5;241m=\u001b[39mtarget_schema\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_subschema \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reordered_schema) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(target_schema):\n",
      "File \u001b[1;32mc:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lancedb\\table.py:320\u001b[0m, in \u001b[0;36m_align_field_types\u001b[1;34m(fields, target_fields)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in target schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_struct(target_field\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m    319\u001b[0m     new_type \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mstruct(\n\u001b[1;32m--> 320\u001b[0m         \u001b[43m_align_field_types\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_field\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m     )\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_list(target_field\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m    326\u001b[0m     new_type \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mlist_(\n\u001b[0;32m    327\u001b[0m         _align_field_types(\n\u001b[0;32m    328\u001b[0m             [field\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mvalue_field],\n\u001b[0;32m    329\u001b[0m             [target_field\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mvalue_field],\n\u001b[0;32m    330\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    331\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Arm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lancedb\\table.py:317\u001b[0m, in \u001b[0;36m_align_field_types\u001b[1;34m(fields, target_fields)\u001b[0m\n\u001b[0;32m    315\u001b[0m target_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m target_fields \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m field\u001b[38;5;241m.\u001b[39mname), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_field \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in target schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_struct(target_field\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m    319\u001b[0m     new_type \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mstruct(\n\u001b[0;32m    320\u001b[0m         _align_field_types(\n\u001b[0;32m    321\u001b[0m             field\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mfields,\n\u001b[0;32m    322\u001b[0m             target_field\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mfields,\n\u001b[0;32m    323\u001b[0m         )\n\u001b[0;32m    324\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Field 'title' not found in target schema"
     ]
    }
   ],
   "source": [
    "table.add(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd8f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
